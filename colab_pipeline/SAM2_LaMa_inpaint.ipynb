{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 + LaMa: Smart Furniture Restoration (Error-Free Version)\n",
    "\n",
    "This notebook fixes the issue where SAM2 removes furniture parts (pillows/cushions).\n",
    "\n",
    "**If you get any import errors:**\n",
    "1. Go to Runtime ‚Üí Restart runtime\n",
    "2. Run all cells again\n",
    "\n",
    "This usually fixes all dependency issues."
   ]
  },
  {
   "cell_type": "code",
   "source": "# @title 1Ô∏è‚É£ Install Dependencies { display-mode: \"form\" }\n# @markdown This cell installs BigLaMa and SAM2 with all required dependencies\n\n# Cell 1: Install with compatible versions\n!pip install -q --upgrade pip\n\n# CRITICAL: Install compatible NumPy version first (before anything else)\n!pip uninstall -y numpy\n!pip install -q numpy==1.24.3\n\n# Clone LaMa repository\n!git clone https://github.com/advimman/lama.git\n\n# Install PyTorch with specific versions for compatibility\n!pip uninstall -y torch torchvision torchaudio\n!pip install -q torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Install core dependencies with specific versions\n!pip install -q opencv-python==4.8.1.78 matplotlib==3.7.2 scipy==1.10.1\n!pip install -q pyyaml==6.0.1 tqdm easydict scikit-image==0.21.0 scikit-learn==1.3.0\n!pip install -q joblib pandas==2.0.3 packaging omegaconf==2.3.0\n\n# Install albumentations 0.5.2 for LaMa compatibility\n!pip uninstall -y albumentations albucore\n!pip install -q albumentations==0.5.2\n\n# Install SAM2 without dependencies\n!pip install -q --no-deps git+https://github.com/facebookresearch/sam2.git\n# Install SAM2 minimal dependencies\n!pip install -q hydra-core==1.3.2 iopath==0.1.10 pillow==9.5.0 submitit==1.5.1\n\n# Install LaMa dependencies\n%cd lama\n!pip install -q pytorch-lightning==1.2.9\n!pip install -q kornia==0.6.7\n!pip install -q webdataset\n!pip install -q wldhx.yadisk-direct\n\n# Download BigLaMa model\nprint(\"Downloading BigLaMa model from HuggingFace...\")\n!curl -LJO https://huggingface.co/smartywu/big-lama/resolve/main/big-lama.zip\n!unzip -q big-lama.zip\n!rm big-lama.zip\n\n%cd ..\n\n# Install UI components\n!pip install -q ipywidgets ipycanvas\n\n# Final check - ensure NumPy didn't get upgraded\n!pip install -q numpy==1.24.3\n\n# Verify installations\nprint(\"\\n‚úÖ Verifying installations:\")\n!python -c \"import numpy; print(f'NumPy: {numpy.__version__}')\"\n!python -c \"import torch; print(f'PyTorch: {torch.__version__}')\"\n!python -c \"import torchvision; print(f'Torchvision: {torchvision.__version__}')\"\n!python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n!pip show albumentations | grep Version\n\nprint(\"\\n‚úÖ Installation complete!\")\nprint(\"BigLaMa is ready to use\")\nprint(\"\\n‚ö†Ô∏è IMPORTANT: Restart runtime now!\")\nprint(\"Runtime ‚Üí Restart runtime\")\nprint(\"Then run all cells in order\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 1.5Ô∏è‚É£ Fix Compatibility Issues { display-mode: \"form\" }\n# @markdown This cell patches the aug.py file to fix albumentations compatibility\n\n# Cell 1.5: Fix DualIAATransform import issue\nprint(\"Fixing albumentations compatibility...\")\n\nimport os\n\n# Create directory if it doesn't exist\nos.makedirs('./lama/saicinpainting/training/data', exist_ok=True)\n\n# Create a patch for the aug.py file\npatch_content = '''from albumentations.core.transforms_interface import DualTransform\nimport imgaug.augmenters as iaa\nimport numpy as np\n\n# Create DualIAATransform if it doesn't exist\ntry:\n    from albumentations import DualIAATransform, to_tuple\nexcept ImportError:\n    from albumentations.core.transforms_interface import to_tuple\n    \n    class DualIAATransform(DualTransform):\n        \"\"\"Base class for IAA transforms.\"\"\"\n        def __init__(self, always_apply=False, p=0.5):\n            super(DualIAATransform, self).__init__(always_apply, p)\n\nclass IAAAffine2(DualIAATransform):\n    \"\"\"Place a regular grid of points on the input and randomly move the neighbourhood of these point around\n    via affine transformations.\n\n    Note:\n        This class introduce interpolation artifacts to mask if it has values other than {0;1}\n\n    Args:\n        scale (float, tuple of float): Scaling factor to use, where 1.0 represents no change and 0.5 is\n            zoomed out to 50 percent of the original size.\n            * If a single float, then that value will be used for all images.\n            * If a tuple (a, b), then a value will be uniformly sampled per image from the interval [a, b].\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then from that parameter per image.\n        translate_percent (float, tuple of float dict-like):\n            Translation as a fraction of the image height/width (x-translation, y-translation),\n            where 0 denotes \"no change\" and 0.5 denotes \"half of the axis size\".\n            * If None, then no translation will be performed.\n            * If a single float, then that value will be used for all images.\n            * If a tuple (a, b), then a value will be uniformly sampled per image from the interval [a, b].\n            * If a list, then a random value will be sampled from that list per image.\n            * If a StochasticParameter, then from that parameter per image.\n            * If a dictionary, then it may contain the keys x and/or y. Each of these keys may have the\n              same datatypes as described above. Using a dictionary allows to set different values for the\n              two axis and sampling will then happen independently per axis, resulting in samples that differ\n              between the axes.\n        translate_px (int, tuple of int dict-like):\n            Translation in pixels.\n        rotate (float, tuple of float):\n            Rotation in degrees (-360 to 360), where 0 denotes \"no change\" and 45 denotes a rotation of\n            45 degrees in clockwise direction.\n        shear (float, tuple of float):\n            Shear in degrees (-360 to 360), where 0 denotes \"no change\".\n        order (int, list of int, str, list of str, iap.ALL): Interpolation order to use. Same meaning as in skimage:\n            * 0: Nearest-neighbor\n            * 1: Bi-linear (default)\n            * 2: Bi-quadratic (not recommended by skimage)\n            * 3: Bi-cubic\n            * 4: Bi-quartic\n            * 5: Bi-quintic\n        cval (float, tuple of float):\n            The constant value to use when filling in newly created pixels.\n        mode (string, list of string): Method to use when filling in newly created pixels.\n            Same meaning as in skimage (and numpy.pad):\n            * 'constant': Pads with a constant value\n            * 'edge': Pads with the edge values of array\n            * 'symmetric': Pads with the reflection of the vector mirrored along the edge of the array.\n            * 'reflect': Pads with the reflection of the vector mirrored on the first and last values of\n              the vector along each axis.\n            * 'wrap': Pads with the wrap of the vector along the axis. The first values are used to pad\n              the end and the end values are used to pad the beginning.\n    Targets:\n        image, mask\n    \"\"\"\n\n    def __init__(\n        self,\n        scale=1.0,\n        translate_percent=None,\n        translate_px=None,\n        rotate=0.0,\n        shear=0.0,\n        order=1,\n        cval=0,\n        mode=\"reflect\",\n        always_apply=False,\n        p=0.5,\n    ):\n        super(IAAAffine2, self).__init__(always_apply, p)\n        self.scale = scale\n        self.translate_percent = translate_percent\n        self.translate_px = translate_px\n        self.rotate = rotate\n        self.shear = shear\n        self.order = order\n        self.cval = cval\n        self.mode = mode\n\n    def apply(self, img, matrix, **params):\n        import cv2\n        import numpy as np\n        return cv2.warpAffine(img, matrix[:2], (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n\n    def apply_to_mask(self, img, matrix, **params):\n        import cv2\n        import numpy as np\n        return cv2.warpAffine(img, matrix[:2], (img.shape[1], img.shape[0]), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_REFLECT_101)\n    \n    def get_params(self):\n        return {\"matrix\": np.eye(3)}\n\n    def get_transform_init_args_names(self):\n        return (\"scale\", \"translate_percent\", \"translate_px\", \"rotate\", \"shear\", \"order\", \"cval\", \"mode\")\n\n\nclass IAAPerspective2(DualIAATransform):\n    \"\"\"Perform a random four point perspective transform of the input.\n\n    Note:\n        This class introduce interpolation artifacts to mask if it has values other than {0;1}\n\n    Args:\n        scale (float, tuple of float): Standard deviation of the normal distributions. These are used to sample\n            the random distances of the subimage's corners from the full image's corners. The sampled values\n            reflect percentage values (with respect to image height/width). Recommended values are in the\n            range 0.0 to 0.1.\n        keep_size (bool): Whether to resize image's back to their original size after applying the\n            perspective transform. If set to False, the resulting images may end up having different shapes\n            and will always be a list, never an array.\n    Targets:\n        image, mask\n    \"\"\"\n\n    def __init__(self, scale=(0.05, 0.1), keep_size=True, always_apply=False, p=0.5):\n        super(IAAPerspective2, self).__init__(always_apply, p)\n        self.scale = scale\n        self.keep_size = keep_size\n\n    def apply(self, img, matrix, max_width, max_height, **params):\n        import cv2\n        return cv2.warpPerspective(img, matrix, (max_width, max_height))\n\n    def apply_to_mask(self, img, matrix, max_width, max_height, **params):\n        import cv2\n        return cv2.warpPerspective(img, matrix, (max_width, max_height))\n\n    def get_params(self):\n        import numpy as np\n        return {\"matrix\": np.eye(3), \"max_width\": 100, \"max_height\": 100}\n\n    def get_transform_init_args_names(self):\n        return (\"scale\", \"keep_size\")\n'''\n\n# Write the patched file\nwith open('./lama/saicinpainting/training/data/aug.py', 'w') as f:\n    f.write(patch_content)\n\nprint(\"‚úÖ Created compatibility patch for aug.py\")\nprint(\"The import error should now be resolved\")"
  },
  {
   "cell_type": "code",
   "source": "# @title 2Ô∏è‚É£ Import Libraries { display-mode: \"form\" }\n# @markdown This cell imports all required libraries for SAM2 and BigLaMa\n\n# Cell 2: Import everything\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntry:\n    import torch\n    import numpy as np\n    import cv2\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    from sam2.sam2_image_predictor import SAM2ImagePredictor\n    \n    # Import LaMa modules\n    import sys\n    sys.path.append('./lama')\n    from saicinpainting.training.trainers import load_checkpoint\n    from saicinpainting.evaluation.utils import move_to_device\n    from saicinpainting.evaluation.refinement import refine_predict\n    import yaml\n    from omegaconf import OmegaConf\n    from saicinpainting.evaluation.data import pad_img_to_modulo\n    \n    from scipy import ndimage\n    from google.colab import files, output\n    output.enable_custom_widget_manager()\n    \n    print(\"‚úÖ All imports successful!\")\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print(f\"Using device: {device}\")\n    \nexcept ImportError as e:\n    print(\"‚ùå Import error:\", e)\n    print(\"\\nPlease:\")\n    print(\"1. Go to Runtime ‚Üí Restart runtime\")\n    print(\"2. Run Cell 1 again\")\n    print(\"3. Then run this cell\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 3Ô∏è‚É£ Load Models { display-mode: \"form\" }\n# @markdown This cell loads SAM2 for segmentation and BigLaMa for inpainting\n\n# Cell 3: Load models with error handling\nprint(\"Loading models...\")\n\n# Import required modules\nimport torch\nimport yaml\nfrom omegaconf import OmegaConf\nimport sys\nsys.path.append('./lama')\nfrom saicinpainting.training.trainers import load_checkpoint\n\n# Try to load SAM2 with error handling\ntry:\n    # Disable JIT for compatibility\n    torch.jit._state.disable()\n    \n    predictor = SAM2ImagePredictor.from_pretrained(\"facebook/sam2-hiera-large\")\n    print(\"‚úÖ SAM2 loaded\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è SAM2 loading issue: {e}\")\n    print(\"Trying alternative loading method...\")\n    \n    # Alternative: Load with different configuration\n    try:\n        from sam2.build_sam import build_sam2\n        from sam2.sam2_image_predictor import SAM2ImagePredictor\n        \n        # Use the base model instead\n        checkpoint = \"facebook/sam2-hiera-base-plus\"\n        predictor = SAM2ImagePredictor.from_pretrained(checkpoint)\n        print(\"‚úÖ SAM2 loaded (base model)\")\n    except:\n        print(\"‚ùå Could not load SAM2. You may need to restart runtime.\")\n        raise\n\n# Load BigLaMa model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Setup config for BigLaMa\npredict_config = OmegaConf.create({\n    'model': {\n        'path': './lama/big-lama/big-lama_places512_G.pth'\n    },\n    'refine': True,\n    'refiner': {\n        'gpu_ids': '0',\n        'modulo': 8,\n        'n_iters': 15,\n        'lr': 0.002,\n        'min_side': 512,\n        'max_scales': 3,\n        'px_budget': 1800000\n    },\n    'out_key': 'inpainted'\n})\n\n# Load model checkpoint\ntrain_config_path = './lama/big-lama/config.yaml'\nwith open(train_config_path, 'r') as f:\n    train_config = OmegaConf.create(yaml.safe_load(f))\n\ntrain_config.training_model.predict_only = True\ntrain_config.visualizer.kind = 'noop'\n\n# Initialize BigLaMa\ncheckpoint_path = predict_config.model.path\nmodel = load_checkpoint(train_config, checkpoint_path, strict=False, map_location='cpu')\nmodel.to(device)\nmodel.eval()\n\nprint(\"‚úÖ BigLaMa loaded\")\nprint(\"   Model: big-lama_places512_G.pth\")\nprint(\"   Refinement: Enabled (15 iterations)\")\n\nprint(\"\\nReady to process images!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 4Ô∏è‚É£ Upload and Display Image { display-mode: \"form\" }\n# @markdown Upload your furniture image and prepare it for processing\n\n# Cell 4: Upload and process image\nprint(\"Upload your furniture image:\")\nuploaded = files.upload()\nfilename = list(uploaded.keys())[0]\n\n# Load image\nimage = Image.open(filename).convert(\"RGB\")\nimage_np = np.array(image)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(image)\nplt.title(f\"Original Image ({image.size[0]}x{image.size[1]})\")\nplt.axis('off')\nplt.show()\n\n# Set image for SAM2\npredictor.set_image(image_np)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 5Ô∏è‚É£ Select Furniture with SAM2 { display-mode: \"form\" }\n# @markdown Click on the furniture to create a segmentation mask\n\n# Cell 5: Simple point selection\nprint(\"Enter coordinates for furniture selection:\")\nprint(\"(You can click on the image above to see coordinates)\")\n\n# Get center point as default\ncenter_x = image.size[0] // 2\ncenter_y = image.size[1] // 2\n\nx = int(input(f\"X coordinate (default {center_x}): \") or center_x)\ny = int(input(f\"Y coordinate (default {center_y}): \") or center_y)\n\n# Generate mask\ncoords = np.array([[x, y]])\nlabels = np.array([1])\n\nmasks, scores, _ = predictor.predict(\n    point_coords=coords,\n    point_labels=labels,\n    multimask_output=False\n)\n\n# Get mask\nmask = (masks[0] > 0.5).astype(np.uint8) * 255\n\n# Show results\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\naxes[0].imshow(image)\naxes[0].plot(x, y, 'go', markersize=15)\naxes[0].set_title(\"Click Point\")\naxes[0].axis('off')\n\naxes[1].imshow(mask, cmap='gray')\naxes[1].set_title(\"SAM2 Mask\")\naxes[1].axis('off')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 6Ô∏è‚É£ Smart Furniture Restoration Preparation { display-mode: \"form\" }\n# @markdown Detect missing furniture parts and prepare for inpainting\n\n# Cell 6: Smart furniture restoration\nprint(\"üîÑ Running smart furniture restoration...\")\n\n# 1. Detect complete furniture boundary\nkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (50, 50))\nclosed_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\nfilled_mask = ndimage.binary_fill_holes(closed_mask).astype(np.uint8) * 255\n\n# 2. Find areas to inpaint (missing parts)\nmissing_parts = cv2.bitwise_and(filled_mask, cv2.bitwise_not(mask))\n\n# 3. Detect white artifacts\nbg_removed = image.copy()\nbg_array = np.array(bg_removed)\nbg_array[mask == 0] = 255  # White background\nbg_removed = Image.fromarray(bg_array)\n\ngray = cv2.cvtColor(bg_array, cv2.COLOR_RGB2GRAY)\nwhite_artifacts = ((gray > 240) & (filled_mask > 0)).astype(np.uint8) * 255\n\n# 4. Combine all areas to inpaint\ninpaint_mask = cv2.bitwise_or(missing_parts, white_artifacts)\nkernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\ninpaint_mask = cv2.dilate(inpaint_mask, kernel_small, iterations=1)\n\n# Show what we'll inpaint\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\naxes[0].imshow(bg_removed)\naxes[0].set_title(\"Background Removed (with holes)\")\naxes[0].axis('off')\n\naxes[1].imshow(inpaint_mask, cmap='gray')\naxes[1].set_title(\"Areas to Restore\")\naxes[1].axis('off')\n\n# Overlay\noverlay = bg_array.copy()\noverlay[inpaint_mask > 0] = [255, 0, 0]\naxes[2].imshow(overlay)\naxes[2].set_title(\"Areas to Restore (Red)\")\naxes[2].axis('off')\nplt.show()\n\nprint(\"‚úÖ Smart mask created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 7Ô∏è‚É£ Run BigLaMa Inpainting { display-mode: \"form\" }\n# @markdown Restore furniture using BigLaMa with refinement\n\n# Cell 7: Run BigLaMa inpainting\nprint(\"üé® Running BigLaMa inpainting with refinement...\")\n\n# Prepare tensors\nimg_tensor = torch.from_numpy(np.array(image)).float().permute(2, 0, 1).unsqueeze(0) / 255.0\nmask_tensor = torch.from_numpy(inpaint_mask).float().unsqueeze(0).unsqueeze(0) / 255.0\n\n# Pad to multiple of 8 for the model\nimg_tensor, mask_tensor = pad_img_to_modulo(img_tensor, mask_tensor, 8)\n\n# Move to device\nbatch = {'image': img_tensor.to(device), 'mask': mask_tensor.to(device)}\n\n# Run BigLaMa with refinement\nprint(\"Running inference with refinement...\")\nwith torch.no_grad():\n    batch = move_to_device(batch, device)\n    batch['unpad_to_size'] = [image.size[1], image.size[0]]\n    \n    # First pass\n    batch = model(batch)\n    \n    # Refinement for best quality\n    if predict_config.refine:\n        batch = refine_predict(batch, model, **predict_config.refiner)\n        cur_res = batch[predict_config.out_key][0].permute(1, 2, 0).detach().cpu().numpy()\n    else:\n        cur_res = batch['inpainted'][0].permute(1, 2, 0).detach().cpu().numpy()\n\n# Convert back to image\ncur_res = np.clip(cur_res * 255, 0, 255).astype('uint8')\ncur_res = cur_res[:image.size[1], :image.size[0]]  # Crop to original size\nresult = Image.fromarray(cur_res)\n\n# Apply furniture mask to remove background\nfinal = Image.new('RGB', result.size, (255, 255, 255))\nresult_array = np.array(result)\nfinal_array = np.array(final)\nfinal_array[filled_mask > 0] = result_array[filled_mask > 0]\nfinal = Image.fromarray(final_array)\n\nprint(\"‚úÖ BigLaMa inpainting complete!\")\nprint(\"   - Used 15 refinement iterations\")\nprint(\"   - Maximum quality output\")\n\n# Show results\nfig, axes = plt.subplots(1, 3, figsize=(20, 8))\n\naxes[0].imshow(image)\naxes[0].set_title(\"Original\", fontsize=16)\naxes[0].axis('off')\n\naxes[1].imshow(bg_removed)\naxes[1].set_title(\"SAM2 Result (with holes)\", fontsize=16)\naxes[1].axis('off')\n\naxes[2].imshow(final)\naxes[2].set_title(\"BigLaMa Restored (Maximum Quality)\", fontsize=16)\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 8Ô∏è‚É£ Save and Download Results { display-mode: \"form\" }\n# @markdown Save the restored image and comparison\n\n# Cell 8: Save results\nbase_name = filename.rsplit('.', 1)[0]\n\n# Save restored image\nrestored_name = f\"{base_name}_restored.png\"\nfinal.save(restored_name)\nprint(f\"‚úÖ Saved: {restored_name}\")\n\n# Save comparison\ncomparison = Image.new('RGB', (image.width * 3, image.height))\ncomparison.paste(image, (0, 0))\ncomparison.paste(bg_removed, (image.width, 0))\ncomparison.paste(final, (image.width * 2, 0))\n\ncomparison_name = f\"{base_name}_comparison.jpg\"\ncomparison.save(comparison_name)\nprint(f\"‚úÖ Saved: {comparison_name}\")\n\n# Download\nfiles.download(restored_name)\nfiles.download(comparison_name)\n\nprint(\"\\n‚úÖ Smart furniture restoration complete!\")\nprint(\"   - Restored missing pillows/cushions\")\nprint(\"   - Removed white artifacts\")\nprint(\"   - Preserved furniture structure\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}