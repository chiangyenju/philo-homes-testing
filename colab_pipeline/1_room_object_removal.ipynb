{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Room Object Removal Pipeline\n",
    "Remove furniture and objects from room photos to create empty room images using YOLO + SAM + LaMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup - Install packages and download models\n",
    "!pip install ultralytics>=8.0.0 opencv-python>=4.8.0 segment-anything lama-cleaner\n",
    "!pip install torch torchvision numpy pillow matplotlib\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries and configure parameters\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from google.colab import files\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from lama_cleaner.model.lama import LaMa\n",
    "from lama_cleaner.schema import Config, HDStrategy\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ===== CONFIGURATION PARAMETERS =====\n",
    "# Adjust these to fine-tune results\n",
    "\n",
    "# Detection\n",
    "YOLO_MODEL = 'yolov8l.pt'  # Options: yolov8n, yolov8s, yolov8m, yolov8l, yolov8x\n",
    "CONFIDENCE_THRESHOLD = 0.002\n",
    "\n",
    "# Objects to remove\n",
    "REMOVE_CLASSES = ['person', 'chair', 'couch', 'bed', 'dining table', 'toilet', \n",
    "                  'tv', 'laptop', 'mouse', 'keyboard', 'cell phone', 'book',\n",
    "                  'clock', 'vase', 'teddy bear', 'potted plant', 'suitcase',\n",
    "                  'handbag', 'backpack', 'umbrella', 'bottle', 'cup', 'bowl',\n",
    "                  'refrigerator', 'oven', 'microwave', 'toaster', 'sink']\n",
    "\n",
    "# Mask processing\n",
    "DILATE_KERNEL_SIZE = 20\n",
    "DILATE_ITERATIONS = 7\n",
    "\n",
    "# Color correction\n",
    "LIGHTNESS_CORRECTION = 1\n",
    "COLOR_CORRECTION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Upload and load image\n",
    "uploaded = files.upload()\n",
    "image_path = list(uploaded.keys())[0]\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(image_path)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image_rgb)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {image_rgb.shape[1]} x {image_rgb.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Detect objects with smart filtering\n",
    "model = YOLO(YOLO_MODEL)\n",
    "results = model(image_rgb, conf=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "# Smart filtering to avoid removing architectural features\n",
    "detections = []\n",
    "excluded = []\n",
    "\n",
    "for r in results:\n",
    "    for box in r.boxes:\n",
    "        class_name = r.names[int(box.cls)]\n",
    "        confidence = float(box.conf)\n",
    "        \n",
    "        if class_name in REMOVE_CLASSES:\n",
    "            bbox = box.xyxy[0].cpu().numpy()\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            h, w = image_rgb.shape[:2]\n",
    "            \n",
    "            # Calculate useful metrics\n",
    "            obj_height = y2 - y1\n",
    "            obj_width = x2 - x1\n",
    "            aspect_ratio = obj_height / obj_width if obj_width > 0 else 0\n",
    "            area_ratio = (obj_height * obj_width) / (h * w)\n",
    "            center_y = (y1 + y2) / 2\n",
    "            \n",
    "            exclude = False\n",
    "            reason = \"\"\n",
    "            \n",
    "            # TV â†’ Window detection\n",
    "            if class_name == 'tv':\n",
    "                if y1 < h * 0.5 and confidence < 0.5:  # Upper half + low confidence\n",
    "                    exclude = True\n",
    "                    reason = \"Likely window (position)\"\n",
    "                elif aspect_ratio < 0.5 and y1 < h * 0.6:  # Wide and high up\n",
    "                    exclude = True\n",
    "                    reason = \"Window-like dimensions\"\n",
    "                    \n",
    "            # Refrigerator â†’ Door/Wall detection\n",
    "            elif class_name == 'refrigerator':\n",
    "                if aspect_ratio > 2.5:  # Very tall and narrow\n",
    "                    exclude = True\n",
    "                    reason = \"Door-like dimensions\"\n",
    "                elif area_ratio > 0.3:  # Takes up too much space\n",
    "                    exclude = True\n",
    "                    reason = \"Wall-sized\"\n",
    "                elif confidence < 0.3 and x1 < 50:  # Low confidence near edge\n",
    "                    exclude = True\n",
    "                    reason = \"Likely wall/door\"\n",
    "            \n",
    "            # Oven/Microwave â†’ Often architectural\n",
    "            elif class_name in ['oven', 'microwave']:\n",
    "                if confidence < 0.4:\n",
    "                    exclude = True\n",
    "                    reason = \"Low confidence architectural\"\n",
    "                elif y1 < h * 0.3:  # Too high for appliances\n",
    "                    exclude = True\n",
    "                    reason = \"Position suggests window\"\n",
    "            \n",
    "            # Sink â†’ Sometimes windows\n",
    "            elif class_name == 'sink':\n",
    "                if y1 < h * 0.4 and confidence < 0.5:\n",
    "                    exclude = True\n",
    "                    reason = \"High position suggests window\"\n",
    "            \n",
    "            # Bed â†’ Sometimes floors\n",
    "            elif class_name == 'bed':\n",
    "                if area_ratio > 0.5:  # Covers most of image\n",
    "                    exclude = True\n",
    "                    reason = \"Likely floor misdetection\"\n",
    "            \n",
    "            # Dining table â†’ Sometimes floors\n",
    "            elif class_name == 'dining table':\n",
    "                if area_ratio > 0.4 and center_y > h * 0.7:\n",
    "                    exclude = True\n",
    "                    reason = \"Likely floor pattern\"\n",
    "            \n",
    "            if exclude:\n",
    "                excluded.append({\n",
    "                    'bbox': bbox,\n",
    "                    'class': class_name,\n",
    "                    'conf': confidence,\n",
    "                    'reason': reason\n",
    "                })\n",
    "            else:\n",
    "                detections.append({\n",
    "                    'bbox': bbox,\n",
    "                    'class': class_name,\n",
    "                    'conf': confidence\n",
    "                })\n",
    "\n",
    "print(f\"\\nðŸ“Š Detection Summary:\")\n",
    "print(f\"  âœ“ Objects to remove: {len(detections)}\")\n",
    "print(f\"  âœ— Protected areas: {len(excluded)}\")\n",
    "\n",
    "# Visualize\n",
    "annotated = results[0].plot()\n",
    "\n",
    "# Draw excluded items in yellow with reason\n",
    "for exc in excluded:\n",
    "    x1, y1, x2, y2 = exc['bbox'].astype(int)\n",
    "    cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 255), 3)\n",
    "    label = f\"KEEP: {exc['reason']}\"\n",
    "    cv2.putText(annotated, label, (x1, y1-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(annotated)\n",
    "plt.title(\"Smart Object Detection (Yellow = Protected from removal)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Detailed report\n",
    "if excluded:\n",
    "    print(\"\\nðŸ›¡ï¸ Protected from removal:\")\n",
    "    for e in excluded:\n",
    "        print(f\"  - {e['class']} ({e['conf']:.2f}) â†’ {e['reason']}\")\n",
    "\n",
    "if detections:\n",
    "    print(\"\\nðŸ—‘ï¸ Will remove:\")\n",
    "    object_counts = {}\n",
    "    for d in detections:\n",
    "        obj_class = d['class']\n",
    "        object_counts[obj_class] = object_counts.get(obj_class, 0) + 1\n",
    "    for obj_class, count in sorted(object_counts.items()):\n",
    "        print(f\"  - {count}x {obj_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create precise masks using SAM\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b_01ec64.pth\")\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image_rgb)\n",
    "\n",
    "# Create combined mask\n",
    "h, w = image_rgb.shape[:2]\n",
    "combined_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "for detection in detections:\n",
    "    bbox = detection['bbox'].astype(int)\n",
    "    input_box = np.array([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "    \n",
    "    masks, _, _ = predictor.predict(\n",
    "        box=input_box,\n",
    "        multimask_output=True\n",
    "    )\n",
    "    \n",
    "    best_mask = masks[np.argmax(masks.sum(axis=(1, 2)))]\n",
    "    combined_mask = np.logical_or(combined_mask, best_mask).astype(np.uint8)\n",
    "\n",
    "# Convert and dilate\n",
    "combined_mask = combined_mask * 255\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (DILATE_KERNEL_SIZE, DILATE_KERNEL_SIZE))\n",
    "dilated_mask = cv2.dilate(combined_mask, kernel, iterations=DILATE_ITERATIONS)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(dilated_mask, cmap='gray')\n",
    "plt.title(\"Objects to Remove (Mask)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Quick preview with OpenCV inpainting\n",
    "inpainted = cv2.inpaint(image_rgb, dilated_mask, 5, cv2.INPAINT_NS)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_rgb)\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(inpainted)\n",
    "plt.title(\"OpenCV Inpainting (Quick Preview)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: High-quality inpainting with LaMa\n",
    "print(\"Loading LaMa model...\")\n",
    "lama_model = LaMa(device)\n",
    "\n",
    "# LaMa configuration\n",
    "config = Config(\n",
    "    ldm_steps=1,\n",
    "    ldm_sampler='plms',\n",
    "    hd_strategy=HDStrategy.RESIZE,\n",
    "    hd_strategy_crop_margin=32,\n",
    "    hd_strategy_crop_trigger_size=800,\n",
    "    hd_strategy_resize_limit=1024,\n",
    ")\n",
    "\n",
    "# Ensure binary mask\n",
    "binary_mask = (dilated_mask > 127).astype(np.uint8) * 255\n",
    "\n",
    "# Run LaMa\n",
    "print(\"Running LaMa inpainting...\")\n",
    "lama_result = lama_model(image_rgb, binary_mask, config)\n",
    "\n",
    "# Fix format conversion\n",
    "if lama_result.dtype != np.uint8:\n",
    "    if lama_result.max() <= 1.0:\n",
    "        lama_result = (lama_result * 255).astype(np.uint8)\n",
    "    else:\n",
    "        lama_result = np.clip(lama_result, 0, 255).astype(np.uint8)\n",
    "\n",
    "print(f\"LaMa result: dtype={lama_result.dtype}, range=[{lama_result.min()}, {lama_result.max()}]\")\n",
    "\n",
    "# Color correction\n",
    "lama_lab = cv2.cvtColor(lama_result, cv2.COLOR_RGB2LAB).astype(float)\n",
    "original_lab = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2LAB).astype(float)\n",
    "\n",
    "mask_inv = cv2.bitwise_not(binary_mask)\n",
    "for i in range(3):\n",
    "    original_mean = original_lab[:,:,i][mask_inv > 0].mean()\n",
    "    lama_mean = lama_lab[:,:,i].mean()\n",
    "    \n",
    "    if i == 0:  # Lightness\n",
    "        shift = original_mean - lama_mean\n",
    "        lama_lab[:,:,i] += shift * LIGHTNESS_CORRECTION\n",
    "    else:  # Color\n",
    "        shift = original_mean - lama_mean  \n",
    "        lama_lab[:,:,i] += shift * COLOR_CORRECTION\n",
    "\n",
    "lama_lab = np.clip(lama_lab, 0, 255)\n",
    "color_corrected = cv2.cvtColor(lama_lab.astype(np.uint8), cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image_rgb)\n",
    "plt.title(\"Original\", fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(lama_result)\n",
    "plt.title(\"LaMa Result\", fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(color_corrected)\n",
    "plt.title(\"Color Corrected\", fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_result = color_corrected\n",
    "print(\"First pass complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save the cleaned room image\n",
    "# Save the result\n",
    "output_path = 'empty_room.jpg'\n",
    "cv2.imwrite(output_path, cv2.cvtColor(best_result, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Download the result\n",
    "files.download(output_path)\n",
    "\n",
    "print(f\"Empty room image saved as: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
