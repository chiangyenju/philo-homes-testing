{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAM2 + PowerPaint V2 Inpainting\n",
    "State-of-the-art inpainting using PowerPaint V2 - currently one of the best models\n",
    "\n",
    "PowerPaint advantages:\n",
    "- Trained on high-quality datasets\n",
    "- Better than LaMa for complex textures\n",
    "- Excellent structure understanding\n",
    "- Works great with furniture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "# First, install compatible torch version\n",
    "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124 --quiet\n",
    "\n",
    "# Then install other dependencies\n",
    "!pip install git+https://github.com/facebookresearch/sam2.git --quiet\n",
    "!pip install opencv-python pillow numpy matplotlib ipywidgets ipycanvas --quiet\n",
    "!pip install transformers diffusers accelerate --quiet\n",
    "!pip install xformers --quiet  # Optional, will work without it\n",
    "!pip install scipy scikit-image --quiet  # For smart restoration\n",
    "\n",
    "from google.colab import files, output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from ipycanvas import MultiCanvas\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from io import BytesIO\n",
    "from scipy import ndimage\n",
    "from skimage import morphology\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    \n",
    "    # Enable optimizations\n",
    "    if 'A100' in gpu_name or 'V100' in gpu_name:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load models\n",
    "print(\"Loading SAM2 model...\")\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\n",
    "    \"facebook/sam2-hiera-large\", \n",
    "    mask_threshold=0.0\n",
    ")\n",
    "\n",
    "print(\"Loading inpainting model...\")\n",
    "# Using a high-quality inpainting model\n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ").to(device)\n",
    "\n",
    "# Enable memory efficient attention if available\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    print(\"✅ XFormers enabled for faster inference\")\n",
    "except:\n",
    "    print(\"⚠️ XFormers not available, using default attention\")\n",
    "    \n",
    "# Enable other optimizations\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "print(\"✅ Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Upload image\n",
    "print(\"Upload furniture image with background:\")\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load image\n",
    "pil_image = Image.open(filename).convert(\"RGB\")\n",
    "image_np = np.array(pil_image)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image_np)\n",
    "plt.title(f\"Original Image ({pil_image.size[0]}x{pil_image.size[1]})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Set image for SAM2\n",
    "predictor.set_image(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Interactive point selection for SAM2\n",
    "from ipywidgets import Image as IPYImage\n",
    "\n",
    "print(\"🎯 Click on the MAIN BODY of the furniture (avoid pillows/cushions)\")\n",
    "print(\"   This helps identify the core furniture structure\")\n",
    "\n",
    "# Setup canvas\n",
    "w, h = pil_image.size\n",
    "f = BytesIO()\n",
    "pil_image.save(f, format='PNG')\n",
    "f.seek(0)\n",
    "image_widget = IPYImage(value=f.read(), format='png', width=w, height=h)\n",
    "\n",
    "canvases = MultiCanvas(2, width=w, height=h)\n",
    "display(canvases)\n",
    "base, overlay = canvases[0], canvases[1]\n",
    "base.draw_image(image_widget, 0, 0)\n",
    "\n",
    "# Point storage\n",
    "points = []\n",
    "labels = []  # 1 for positive, 0 for negative\n",
    "\n",
    "# UI controls\n",
    "point_type = widgets.RadioButtons(\n",
    "    options=[('Include (Green)', 1), ('Exclude (Red)', 0)],\n",
    "    value=1,\n",
    "    description='Point Type:'\n",
    ")\n",
    "clear_points_btn = widgets.Button(description='Clear Points')\n",
    "point_count = widgets.Label(value='Points: 0')\n",
    "\n",
    "display(widgets.HBox([point_type, clear_points_btn, point_count]))\n",
    "\n",
    "def on_mouse_down(x, y):\n",
    "    label = point_type.value\n",
    "    points.append((x, y))\n",
    "    labels.append(label)\n",
    "    \n",
    "    # Draw point\n",
    "    color = 'lime' if label == 1 else 'red'\n",
    "    overlay.fill_style = color\n",
    "    overlay.fill_circle(x, y, 8)\n",
    "    \n",
    "    # Update count display\n",
    "    point_count.value = f'Points: {len(points)}'\n",
    "\n",
    "def clear_points(b):\n",
    "    global points, labels\n",
    "    points = []\n",
    "    labels = []\n",
    "    overlay.clear()\n",
    "    point_count.value = 'Points: 0'\n",
    "\n",
    "overlay.on_mouse_down(on_mouse_down)\n",
    "clear_points_btn.on_click(clear_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Generate initial SAM2 mask\n",
    "if not points:\n",
    "    print(\"⚠️ Please click on the furniture first!\")\n",
    "else:\n",
    "    # Convert points for SAM2\n",
    "    coords = np.array(points)\n",
    "    point_labels = np.array(labels)\n",
    "    \n",
    "    # Generate mask\n",
    "    masks, scores, _ = predictor.predict(\n",
    "        point_coords=coords,\n",
    "        point_labels=point_labels,\n",
    "        multimask_output=False\n",
    "    )\n",
    "    \n",
    "    # Get binary mask\n",
    "    initial_mask = (masks[0] > 0.5).astype(np.uint8) * 255\n",
    "    \n",
    "    # Create initial result with white background\n",
    "    bg_removed = Image.new('RGBA', pil_image.size, (255, 255, 255, 255))\n",
    "    furniture_only = pil_image.copy()\n",
    "    furniture_only.putalpha(Image.fromarray(initial_mask))\n",
    "    bg_removed.paste(furniture_only, (0, 0), furniture_only)\n",
    "    bg_removed = bg_removed.convert('RGB')\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(image_np)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(initial_mask, cmap='gray')\n",
    "    axes[1].set_title(\"Initial SAM2 Mask\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(bg_removed)\n",
    "    axes[2].set_title(\"Background Removed (with holes)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Notice the white holes where pillows/cushions were removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Smart furniture boundary detection\n",
    "def detect_complete_furniture_boundary(image, initial_mask):\n",
    "    \"\"\"\n",
    "    Detect the complete furniture boundary including areas that were removed\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 1. Find the convex hull of the initial mask to get overall shape\n",
    "    contours, _ = cv2.findContours(initial_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        hull = cv2.convexHull(largest_contour)\n",
    "        \n",
    "        # Create convex hull mask\n",
    "        hull_mask = np.zeros_like(initial_mask)\n",
    "        cv2.fillPoly(hull_mask, [hull], 255)\n",
    "    else:\n",
    "        hull_mask = initial_mask.copy()\n",
    "    \n",
    "    # 2. Use morphological operations to fill gaps\n",
    "    # Large closing to connect separated parts\n",
    "    kernel_large = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (50, 50))\n",
    "    closed_mask = cv2.morphologyEx(initial_mask, cv2.MORPH_CLOSE, kernel_large, iterations=2)\n",
    "    \n",
    "    # 3. Fill holes inside the furniture\n",
    "    filled_mask = ndimage.binary_fill_holes(closed_mask).astype(np.uint8) * 255\n",
    "    \n",
    "    # 4. Combine with edge detection to refine boundaries\n",
    "    edges = cv2.Canny(gray, 30, 100)\n",
    "    edge_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    edges_dilated = cv2.dilate(edges, edge_kernel, iterations=1)\n",
    "    \n",
    "    # 5. Create final furniture boundary\n",
    "    # Start with filled mask\n",
    "    furniture_boundary = filled_mask.copy()\n",
    "    \n",
    "    # Refine with hull (but don't expand too much)\n",
    "    overlap = cv2.bitwise_and(hull_mask, filled_mask)\n",
    "    furniture_boundary = cv2.bitwise_or(furniture_boundary, overlap)\n",
    "    \n",
    "    # Smooth the boundary\n",
    "    kernel_smooth = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))\n",
    "    furniture_boundary = cv2.morphologyEx(furniture_boundary, cv2.MORPH_CLOSE, kernel_smooth)\n",
    "    furniture_boundary = cv2.morphologyEx(furniture_boundary, cv2.MORPH_OPEN, kernel_smooth)\n",
    "    \n",
    "    return furniture_boundary, hull_mask, filled_mask\n",
    "\n",
    "# Detect complete furniture boundary\n",
    "print(\"🔍 Detecting complete furniture boundary...\")\n",
    "furniture_boundary, hull_mask, filled_mask = detect_complete_furniture_boundary(bg_removed, initial_mask)\n",
    "\n",
    "# Display analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1\n",
    "axes[0, 0].imshow(initial_mask, cmap='gray')\n",
    "axes[0, 0].set_title(\"Initial SAM2 Mask\", fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(hull_mask, cmap='gray')\n",
    "axes[0, 1].set_title(\"Convex Hull\", fontsize=14)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(filled_mask, cmap='gray')\n",
    "axes[0, 2].set_title(\"Filled Mask\", fontsize=14)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2\n",
    "axes[1, 0].imshow(furniture_boundary, cmap='gray')\n",
    "axes[1, 0].set_title(\"Complete Furniture Boundary\", fontsize=14)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Show what needs to be inpainted\n",
    "inpaint_areas = cv2.bitwise_and(furniture_boundary, cv2.bitwise_not(initial_mask))\n",
    "axes[1, 1].imshow(inpaint_areas, cmap='gray')\n",
    "axes[1, 1].set_title(\"Areas to Restore\", fontsize=14)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Overlay on original\n",
    "overlay = bg_removed.copy()\n",
    "overlay_array = np.array(overlay)\n",
    "overlay_array[inpaint_areas > 0] = [255, 0, 0]\n",
    "axes[1, 2].imshow(overlay_array)\n",
    "axes[1, 2].set_title(\"Areas to Restore (Red)\", fontsize=14)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Identified complete furniture boundary and missing areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create smart inpainting mask\n",
    "def create_smart_inpaint_mask(image, initial_mask, furniture_boundary):\n",
    "    \"\"\"\n",
    "    Create an intelligent mask for inpainting that includes:\n",
    "    1. Missing furniture parts (pillows, cushions)\n",
    "    2. White artifacts and spots\n",
    "    3. Edge refinements\n",
    "    \"\"\"\n",
    "    img_array = np.array(image)\n",
    "    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 1. Missing parts mask (areas inside furniture boundary but not in initial mask)\n",
    "    missing_parts = cv2.bitwise_and(furniture_boundary, cv2.bitwise_not(initial_mask))\n",
    "    \n",
    "    # 2. White artifacts detection\n",
    "    # Detect pure white areas within the furniture\n",
    "    white_threshold = 240\n",
    "    white_areas = (gray > white_threshold).astype(np.uint8) * 255\n",
    "    white_artifacts = cv2.bitwise_and(white_areas, furniture_boundary)\n",
    "    \n",
    "    # 3. Small white spots and dots\n",
    "    # Use adaptive thresholding to find local bright spots\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 21, -5\n",
    "    )\n",
    "    small_artifacts = cv2.bitwise_and(adaptive_thresh, furniture_boundary)\n",
    "    \n",
    "    # Remove large connected components (keep only small spots)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(small_artifacts, connectivity=8)\n",
    "    small_artifacts_clean = np.zeros_like(small_artifacts)\n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] < 500:  # Small spots only\n",
    "            small_artifacts_clean[labels == i] = 255\n",
    "    \n",
    "    # 4. Edge artifacts\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    edge_artifacts = cv2.dilate(edges, edge_kernel, iterations=1)\n",
    "    edge_artifacts = cv2.bitwise_and(edge_artifacts, furniture_boundary)\n",
    "    \n",
    "    # Combine all masks\n",
    "    final_mask = missing_parts\n",
    "    final_mask = cv2.bitwise_or(final_mask, white_artifacts)\n",
    "    final_mask = cv2.bitwise_or(final_mask, small_artifacts_clean)\n",
    "    \n",
    "    # Add slight dilation for better inpainting\n",
    "    dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10))\n",
    "    final_mask = cv2.dilate(final_mask, dilate_kernel, iterations=1)\n",
    "    \n",
    "    # Ensure mask stays within furniture boundary\n",
    "    final_mask = cv2.bitwise_and(final_mask, furniture_boundary)\n",
    "    \n",
    "    return final_mask, missing_parts, white_artifacts, small_artifacts_clean\n",
    "\n",
    "# Create smart inpainting mask\n",
    "print(\"🎨 Creating smart inpainting mask...\")\n",
    "inpaint_mask, missing_parts, white_artifacts, small_spots = create_smart_inpaint_mask(\n",
    "    bg_removed, initial_mask, furniture_boundary\n",
    ")\n",
    "\n",
    "# Display mask components\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1: Components\n",
    "axes[0, 0].imshow(missing_parts, cmap='gray')\n",
    "axes[0, 0].set_title(\"Missing Parts (Pillows/Cushions)\", fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(white_artifacts, cmap='gray')\n",
    "axes[0, 1].set_title(\"White Artifacts\", fontsize=14)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(small_spots, cmap='gray')\n",
    "axes[0, 2].set_title(\"Small Spots\", fontsize=14)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2: Final mask\n",
    "axes[1, 0].imshow(inpaint_mask, cmap='gray')\n",
    "axes[1, 0].set_title(\"Final Inpainting Mask\", fontsize=14)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Overlay on image\n",
    "overlay = bg_removed.copy()\n",
    "overlay_array = np.array(overlay)\n",
    "overlay_array[inpaint_mask > 0] = [255, 0, 0]\n",
    "axes[1, 1].imshow(overlay_array)\n",
    "axes[1, 1].set_title(\"Areas to Inpaint (Red)\", fontsize=14)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Show furniture with complete boundary\n",
    "complete_furniture = Image.new('RGBA', pil_image.size, (255, 255, 255, 255))\n",
    "furniture_complete = pil_image.copy()\n",
    "furniture_complete.putalpha(Image.fromarray(furniture_boundary))\n",
    "complete_furniture.paste(furniture_complete, (0, 0), furniture_complete)\n",
    "complete_furniture = complete_furniture.convert('RGB')\n",
    "axes[1, 2].imshow(complete_furniture)\n",
    "axes[1, 2].set_title(\"Expected Result Preview\", fontsize=14)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate statistics\n",
    "total_pixels = np.sum(furniture_boundary > 0)\n",
    "inpaint_pixels = np.sum(inpaint_mask > 0)\n",
    "percentage = (inpaint_pixels / total_pixels) * 100 if total_pixels > 0 else 0\n",
    "\n",
    "print(f\"📊 Need to inpaint {percentage:.1f}% of the furniture\")\n",
    "\n",
    "# Also analyze furniture for better prompts\n",
    "def analyze_furniture_context(image, mask):\n",
    "    \"\"\"\n",
    "    Analyze furniture to generate better inpainting prompts\n",
    "    \"\"\"\n",
    "    img_array = np.array(image)\n",
    "    mask_array = np.array(mask)\n",
    "    \n",
    "    # Get non-artifact areas for color analysis\n",
    "    furniture_pixels = img_array[mask_array > 0]\n",
    "    \n",
    "    # Dominant color analysis\n",
    "    avg_color = np.mean(furniture_pixels, axis=0)\n",
    "    \n",
    "    # Color descriptions\n",
    "    if avg_color[0] > avg_color[1] and avg_color[0] > avg_color[2]:\n",
    "        color = \"warm toned\"\n",
    "    elif avg_color[2] > avg_color[0] and avg_color[2] > avg_color[1]:\n",
    "        color = \"cool toned\"\n",
    "    elif np.std(avg_color) < 20:\n",
    "        color = \"neutral gray\"\n",
    "    else:\n",
    "        color = \"neutral\"\n",
    "    \n",
    "    # Texture analysis using gradients\n",
    "    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    gradient_mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # Get texture from non-artifact areas\n",
    "    non_artifact_mask = cv2.bitwise_and(mask_array, cv2.bitwise_not(inpaint_mask))\n",
    "    texture_score = np.mean(gradient_mag[non_artifact_mask > 0])\n",
    "    \n",
    "    if texture_score > 50:\n",
    "        texture = \"highly textured, tufted\"\n",
    "    elif texture_score > 30:\n",
    "        texture = \"moderately textured\"\n",
    "    else:\n",
    "        texture = \"smooth\"\n",
    "    \n",
    "    return color, texture\n",
    "\n",
    "# Analyze furniture\n",
    "color_desc, texture_desc = analyze_furniture_context(bg_removed, furniture_boundary)\n",
    "\n",
    "# Generate smart prompt\n",
    "prompt = f\"{color_desc} furniture surface, {texture_desc}, continuous material, no gaps or holes, professional product photo\"\n",
    "negative_prompt = \"white spots, holes, damaged areas, missing parts, pillows, cushions, objects\"\n",
    "\n",
    "print(f\"\\n🎨 Auto-generated prompt: {prompt}\")\n",
    "print(f\"🚫 Negative prompt: {negative_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Run smart inpainting with two-pass approach\n",
    "print(\"🔄 Running smart furniture restoration with PowerPaint...\")\n",
    "\n",
    "# First pass: Restore missing parts using original image context\n",
    "print(\"\\nPass 1: Restoring from original image...\")\n",
    "\n",
    "# Use the original image for better context\n",
    "image_for_inpaint = pil_image  # Use original instead of bg_removed\n",
    "mask_for_inpaint = Image.fromarray(inpaint_mask)\n",
    "\n",
    "# Get original dimensions\n",
    "original_size = image_for_inpaint.size\n",
    "\n",
    "# Make dimensions divisible by 8\n",
    "width = (original_size[0] // 8) * 8\n",
    "height = (original_size[1] // 8) * 8\n",
    "\n",
    "# Resize if needed\n",
    "if (width, height) != original_size:\n",
    "    image_resized = image_for_inpaint.resize((width, height), Image.LANCZOS)\n",
    "    mask_resized = mask_for_inpaint.resize((width, height), Image.LANCZOS)\n",
    "else:\n",
    "    image_resized = image_for_inpaint\n",
    "    mask_resized = mask_for_inpaint\n",
    "\n",
    "# Run first pass inpainting\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "strength = 0.99\n",
    "\n",
    "result_pass1 = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=image_resized,\n",
    "    mask_image=mask_resized,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    strength=strength,\n",
    "    generator=torch.Generator(device).manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "# Resize back if needed\n",
    "if (width, height) != original_size:\n",
    "    result_pass1 = result_pass1.resize(original_size, Image.LANCZOS)\n",
    "\n",
    "# Apply the complete furniture mask to remove background\n",
    "restored_furniture = Image.new('RGBA', result_pass1.size, (255, 255, 255, 255))\n",
    "result_pass1_rgba = result_pass1.convert('RGBA')\n",
    "result_pass1_rgba.putalpha(Image.fromarray(furniture_boundary))\n",
    "restored_furniture.paste(result_pass1_rgba, (0, 0), result_pass1_rgba)\n",
    "restored_furniture = restored_furniture.convert('RGB')\n",
    "\n",
    "print(\"✅ First pass complete\")\n",
    "\n",
    "# Second pass: Clean up any remaining artifacts\n",
    "print(\"\\nPass 2: Cleaning remaining artifacts...\")\n",
    "\n",
    "# Detect any remaining white spots\n",
    "gray_restored = cv2.cvtColor(np.array(restored_furniture), cv2.COLOR_RGB2GRAY)\n",
    "remaining_white = (gray_restored > 245).astype(np.uint8) * 255\n",
    "remaining_artifacts = cv2.bitwise_and(remaining_white, furniture_boundary)\n",
    "\n",
    "if np.sum(remaining_artifacts) > 0:\n",
    "    print(f\"Found {np.sum(remaining_artifacts > 0)} pixels of remaining artifacts\")\n",
    "    \n",
    "    # Dilate slightly\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    remaining_artifacts = cv2.dilate(remaining_artifacts, kernel, iterations=1)\n",
    "    \n",
    "    # Run second inpainting\n",
    "    remaining_mask = Image.fromarray(remaining_artifacts)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if (width, height) != original_size:\n",
    "        restored_resized = restored_furniture.resize((width, height), Image.LANCZOS)\n",
    "        remaining_mask_resized = remaining_mask.resize((width, height), Image.LANCZOS)\n",
    "    else:\n",
    "        restored_resized = restored_furniture\n",
    "        remaining_mask_resized = remaining_mask\n",
    "    \n",
    "    final_result = pipe(\n",
    "        prompt=prompt + \", perfect quality, no artifacts\",\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=restored_resized,\n",
    "        mask_image=remaining_mask_resized,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=30,\n",
    "        strength=0.8,\n",
    "        generator=torch.Generator(device).manual_seed(123)\n",
    "    ).images[0]\n",
    "    \n",
    "    # Resize back if needed\n",
    "    if (width, height) != original_size:\n",
    "        final_result = final_result.resize(original_size, Image.LANCZOS)\n",
    "else:\n",
    "    final_result = restored_furniture\n",
    "    print(\"No additional artifacts found\")\n",
    "\n",
    "print(\"✅ Smart restoration complete!\")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 15))\n",
    "\n",
    "# Row 1: Process\n",
    "axes[0, 0].imshow(pil_image)\n",
    "axes[0, 0].set_title(\"1. Original Image\", fontsize=16)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(bg_removed)\n",
    "axes[0, 1].set_title(\"2. SAM2 Background Removed\\n(with holes)\", fontsize=16)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(inpaint_mask, cmap='gray')\n",
    "axes[0, 2].set_title(\"3. Smart Inpaint Mask\", fontsize=16)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2: Results\n",
    "axes[1, 0].imshow(restored_furniture)\n",
    "axes[1, 0].set_title(\"4. After First Pass\", fontsize=16)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(final_result)\n",
    "axes[1, 1].set_title(\"5. Final Result\", fontsize=16)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Comparison\n",
    "comparison = Image.new('RGB', (bg_removed.width * 2, bg_removed.height))\n",
    "comparison.paste(bg_removed, (0, 0))\n",
    "comparison.paste(final_result, (bg_removed.width, 0))\n",
    "axes[1, 2].imshow(comparison)\n",
    "axes[1, 2].set_title(\"Before | After\", fontsize=16)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Fine-tune results (optional)\n",
    "# Manual touch-up options\n",
    "refine_btn = widgets.Button(description='Refine Edges', button_style='primary')\n",
    "smooth_btn = widgets.Button(description='Smooth Texture', button_style='info')\n",
    "enhance_btn = widgets.Button(description='Enhance Details', button_style='success')\n",
    "regenerate_btn = widgets.Button(description='Try Different Seed', button_style='warning')\n",
    "\n",
    "display(widgets.HBox([refine_btn, smooth_btn, enhance_btn, regenerate_btn]))\n",
    "\n",
    "current_seed = 42\n",
    "\n",
    "def refine_edges(b):\n",
    "    global final_result\n",
    "    print(\"Refining edges...\")\n",
    "    \n",
    "    # Detect and smooth edges\n",
    "    result_array = np.array(final_result)\n",
    "    \n",
    "    # Apply bilateral filter for edge-preserving smoothing\n",
    "    smoothed = cv2.bilateralFilter(result_array, 9, 75, 75)\n",
    "    \n",
    "    # Blend with original\n",
    "    alpha = 0.7\n",
    "    blended = cv2.addWeighted(result_array, 1-alpha, smoothed, alpha, 0)\n",
    "    \n",
    "    final_result = Image.fromarray(blended)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(final_result)\n",
    "    plt.title(\"Refined Edges\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Edges refined\")\n",
    "\n",
    "def smooth_texture(b):\n",
    "    global final_result\n",
    "    print(\"Smoothing texture...\")\n",
    "    \n",
    "    # Apply gentle Gaussian blur to smooth textures\n",
    "    result_array = np.array(final_result)\n",
    "    smoothed = cv2.GaussianBlur(result_array, (3, 3), 0)\n",
    "    \n",
    "    # Selective smoothing - only in previously inpainted areas\n",
    "    mask_3ch = cv2.cvtColor(inpaint_mask, cv2.COLOR_GRAY2RGB)\n",
    "    mask_norm = mask_3ch / 255.0\n",
    "    \n",
    "    result_smooth = result_array * (1 - mask_norm) + smoothed * mask_norm\n",
    "    final_result = Image.fromarray(result_smooth.astype(np.uint8))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(final_result)\n",
    "    plt.title(\"Smoothed Texture\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Texture smoothed\")\n",
    "\n",
    "def enhance_details(b):\n",
    "    global final_result\n",
    "    print(\"Enhancing details...\")\n",
    "    \n",
    "    # Apply unsharp masking for detail enhancement\n",
    "    result_array = np.array(final_result)\n",
    "    gaussian = cv2.GaussianBlur(result_array, (0, 0), 2.0)\n",
    "    enhanced = cv2.addWeighted(result_array, 1.5, gaussian, -0.5, 0)\n",
    "    \n",
    "    final_result = Image.fromarray(enhanced)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(final_result)\n",
    "    plt.title(\"Enhanced Details\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Details enhanced\")\n",
    "\n",
    "def regenerate_with_new_seed(b):\n",
    "    global final_result, current_seed\n",
    "    current_seed += 100\n",
    "    print(f\"Regenerating with new seed: {current_seed}\")\n",
    "    \n",
    "    # Re-run inpainting with new seed\n",
    "    image_for_inpaint = pil_image\n",
    "    mask_for_inpaint = Image.fromarray(inpaint_mask)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if (width, height) != original_size:\n",
    "        image_resized = image_for_inpaint.resize((width, height), Image.LANCZOS)\n",
    "        mask_resized = mask_for_inpaint.resize((width, height), Image.LANCZOS)\n",
    "    else:\n",
    "        image_resized = image_for_inpaint\n",
    "        mask_resized = mask_for_inpaint\n",
    "    \n",
    "    new_result = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=image_resized,\n",
    "        mask_image=mask_resized,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        strength=strength,\n",
    "        generator=torch.Generator(device).manual_seed(current_seed)\n",
    "    ).images[0]\n",
    "    \n",
    "    # Resize back and apply mask\n",
    "    if (width, height) != original_size:\n",
    "        new_result = new_result.resize(original_size, Image.LANCZOS)\n",
    "    \n",
    "    # Apply furniture mask\n",
    "    restored = Image.new('RGBA', new_result.size, (255, 255, 255, 255))\n",
    "    result_rgba = new_result.convert('RGBA')\n",
    "    result_rgba.putalpha(Image.fromarray(furniture_boundary))\n",
    "    restored.paste(result_rgba, (0, 0), result_rgba)\n",
    "    final_result = restored.convert('RGB')\n",
    "    \n",
    "    # Display comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    axes[0].imshow(bg_removed)\n",
    "    axes[0].set_title(\"Before\")\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(final_result)\n",
    "    axes[1].set_title(f\"After (seed: {current_seed})\")\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Regenerated with new seed\")\n",
    "\n",
    "refine_btn.on_click(refine_edges)\n",
    "smooth_btn.on_click(smooth_texture)\n",
    "enhance_btn.on_click(enhance_details)\n",
    "regenerate_btn.on_click(regenerate_with_new_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save final results\n",
    "print(f\"Original size: {pil_image.size}\")\n",
    "print(f\"Result size: {final_result.size}\")\n",
    "\n",
    "base_name = filename.rsplit('.', 1)[0]\n",
    "\n",
    "# Save restored furniture\n",
    "restored_name = f\"{base_name}_restored.png\"\n",
    "final_result.save(restored_name, quality=95)\n",
    "print(f\"✅ Saved: {restored_name}\")\n",
    "\n",
    "# Save with transparent background\n",
    "transparent = Image.new('RGBA', final_result.size, (0, 0, 0, 0))\n",
    "final_rgba = final_result.convert('RGBA')\n",
    "transparent.paste(final_rgba, (0, 0), Image.fromarray(furniture_boundary))\n",
    "transparent_name = f\"{base_name}_transparent.png\"\n",
    "transparent.save(transparent_name)\n",
    "print(f\"✅ Saved: {transparent_name}\")\n",
    "\n",
    "# Create detailed comparison\n",
    "comparison = Image.new('RGB', (pil_image.width * 3, pil_image.height))\n",
    "comparison.paste(pil_image, (0, 0))\n",
    "comparison.paste(bg_removed, (pil_image.width, 0))\n",
    "comparison.paste(final_result, (pil_image.width * 2, 0))\n",
    "\n",
    "# Add labels\n",
    "from PIL import ImageDraw, ImageFont\n",
    "draw = ImageDraw.Draw(comparison)\n",
    "try:\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", 50)\n",
    "except:\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "labels = [\"Original\", \"SAM2 Removed BG\", \"Smart Restored\"]\n",
    "for i, label in enumerate(labels):\n",
    "    x = i * pil_image.width + 30\n",
    "    y = 30\n",
    "    # Add shadow\n",
    "    draw.text((x+2, y+2), label, fill='black', font=font)\n",
    "    draw.text((x, y), label, fill='white', font=font)\n",
    "\n",
    "comparison_name = f\"{base_name}_smart_restoration.jpg\"\n",
    "comparison.save(comparison_name, quality=95)\n",
    "print(f\"✅ Saved: {comparison_name}\")\n",
    "\n",
    "# Download\n",
    "files.download(restored_name)\n",
    "files.download(transparent_name)\n",
    "files.download(comparison_name)\n",
    "\n",
    "# Final display\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.imshow(comparison)\n",
    "plt.title(\"Smart Furniture Restoration Complete\", fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Smart furniture restoration complete!\")\n",
    "print(\"   - Restored missing pillows/cushions\")\n",
    "print(\"   - Removed white artifacts and spots\")\n",
    "print(\"   - Preserved furniture structure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
